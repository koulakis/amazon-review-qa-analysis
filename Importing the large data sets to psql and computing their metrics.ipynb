{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the large datasets to a postgresql server and computing their metrics\n",
    "\n",
    "It is not possible to load the larger data sets in the memory of a local machine therefeore an alternative is to import them to a psql table and query them from there. By adding the right indices this can make the queries fast enough. After this import one can extract some basic statistics using sql and also export smaller portions of the data which can be handled by spark or pandas on a local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "def stopwatch(function):\n",
    "    start_time = timeit.default_timer()\n",
    "    result = function()\n",
    "    print('Elapsed time: %i sec' % int(timeit.default_timer() - start_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzipping the data and converting it to csv format\n",
    "\n",
    "Unfortunately psql does not support an import of record json files therefore we need to convert the data sets to csv. We use here the command line tool [json2csv](https://github.com/jehiah/json2csv).\n",
    "\n",
    "**WARNING:** The following two commands will run for a while, especially the second one. You can expect approximately **1 minute per GB** of unzipped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 178 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "!ls ./data/large-datasets/*.gz | grep -Po '.*(?=.gz)' | xargs -I {} gunzip {}.gz\n",
    "\n",
    "print('Elapsed time: %i sec' % int(timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 275 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "!ls ./data/large-datasets/*.json | xargs sed -i 's/|/?/g;s/\\u0000/?/g'\n",
    "\n",
    "print('Elapsed time: %i sec' % int(timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017/08/28 00:16:16 ERROR Decoding JSON at line 8789077: invalid character '?' in string escape code\n",
      "{\"reviewerID\": \"AWGWD8R8PLWH3\", \"asin\": \"B00JL1H75A\", \"reviewerName\": \"Kim\", \"helpful\": [0, 0], \"reviewText\": \"Love this series! I even like Elijah. He has been through a lot and it sounds like his father is some kind of crazy tyrant or despot. I'm thinking along the lines of Saddom Hussein. He really sounds evil. I like Elijah and Natalie together and really hope she can turn him around. I do think there is a chance for them. I just got the 4 th book and now I have to decide if I can wait til the box set comes out. Great Book, and it is hot. He is a little finish and takes over Natalie's life and is very controlling. But as their relationship sort of grows, you get little glimps\\?s of something deeper and better for them. You should read this book. It is really good.\", \"overall\": 5.0, \"summary\": \"Wow\", \"unixReviewTime\": 1403827200, \"reviewTime\": \"06 27, 2014\"}\n",
      "Elapsed time: 735 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "!ls ./data/large-datasets/*.json | grep -Po '.*(?=.json)' | xargs -I {} json2csv -p -d '|' -k asin,helpful,overall,reviewText,reviewTime,reviewerID,reviewerName,summary,unixReviewTime -i {}.json -o {}.csv\n",
    "!rm ./data/large-datasets/*.json\n",
    "\n",
    "print('Elapsed time: %i sec' % int(timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data in psql\n",
    "\n",
    "To import the data in psql we create a table with the appropriate shape and import form the csv files generated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some preparation to run psql transactions and queries in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "\n",
    "db_conf = { \n",
    "    'user': 'mariosk',\n",
    "    'database': 'amazon_reviews'\n",
    "}\n",
    "\n",
    "connection_factory = lambda: pg.connect(user=db_conf['user'], database=db_conf['database'])\n",
    "\n",
    "def transaction(*statements):\n",
    "    try:\n",
    "        connection = connection_factory()\n",
    "        cursor = connection.cursor()\n",
    "        for statement in statements:\n",
    "            cursor.execute(statement)\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "    except pg.DatabaseError as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if connection is not None:\n",
    "            connection.close()\n",
    "    \n",
    "def query(statement):\n",
    "    try:\n",
    "        connection = connection_factory()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(statement)\n",
    "        \n",
    "        header = [ description[0] for description in cursor.description ]\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        cursor.close()\n",
    "        return pd.DataFrame.from_records(rows, columns=header)\n",
    "    except (Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "        return None\n",
    "    finally:\n",
    "        if connection is not None:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tables for with indices for the large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "table_names = [ re.search('reviews_(.*)_5.csv', filename).group(1) \n",
    "    for filename \n",
    "    in sorted(os.listdir('./data/large-datasets'))\n",
    "    if not filename.endswith('json') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_table(table_name):\n",
    "    transaction(\n",
    "        'create table %s (asin text, helpful text, overall double precision, reviewText text, reviewTime text, reviewerID text, reviewerName text, summary text, unixReviewTime int);' % table_name,\n",
    "        'create index {0}_asin ON {0} (asin);'.format(table_name),\n",
    "        'create index {0}_overall ON {0} (overall);'.format(table_name),\n",
    "        'create index {0}_reviewerID ON {0} (reviewerID);'.format(table_name),\n",
    "        'create index {0}_unixReviewTime ON {0} (unixReviewTime);'.format(table_name))\n",
    "\n",
    "for table_name in table_names:\n",
    "    create_table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the datasets to psql\n",
    "\n",
    "**WARNING:** The following command will take long time to complete. Estimate ~1 minute for each GB of csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY 8898040\n",
      "COPY 1097592\n",
      "COPY 1689188\n",
      "COPY 1697533\n",
      "Elapsed time: 871 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "!ls ./data/large-datasets | grep -Po '(?<=reviews_).*(?=_5.csv)' | xargs -I {} psql -U mariosk -d amazon_reviews -c \"\\copy {} from './data/large-datasets/reviews_{}_5.csv' with (format csv, delimiter '|', header true);\"\n",
    "\n",
    "print('Elapsed time: %i sec' % int(timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_reviews_per_product(table_name):\n",
    "    return (query('''\n",
    "        with distinct_products as (select count(distinct asin) as products from {0}),\n",
    "             reviews_count as (select cast(count(*) as double precision) as reviews from {0})\n",
    "        select reviews / products as reviews_per_product\n",
    "        from distinct_products cross join reviews_count\n",
    "    '''.format(table_name))\n",
    "    .rename(index={0: table_name.replace('_', ' ')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_reviews_per_reviewer(table_name):\n",
    "    return (query('''\n",
    "        with distinct_reviewers as (select count(distinct reviewerID) as reviewers from {0}),\n",
    "             reviews_count as (select cast(count(*) as double precision) as reviews from {0})\n",
    "        select reviews / reviewers as reviews_per_reviewer\n",
    "        from distinct_reviewers cross join reviews_count\n",
    "    '''.format(table_name))\n",
    "    .rename(index={ 0: table_name.replace('_', ' ')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percentages_per_rating(table_name):\n",
    "    return (query('''\n",
    "            with rating_counts as (select overall, count(overall) as rating_count from {0} group by overall),\n",
    "                 reviews_count as (select cast(count(*) as double precision) as reviews from {0})\n",
    "            select cast(overall as int) as dataset_name, rating_count / reviews as row\n",
    "            from rating_counts cross join reviews_count\n",
    "        '''.format(table_name))\n",
    "        .set_index('dataset_name')\n",
    "        .sort_index()\n",
    "        .transpose()\n",
    "        .rename(index={'row': table_name.replace('_', ' ')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_reviews(table_name):\n",
    "    return (query('''\n",
    "        select count(*) as number_of_reviews from {0}\n",
    "    '''.format(table_name))\n",
    "    .rename(index={ 0: table_name.replace('_', ' ') }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_metrics(table_name):\n",
    "    print(table_name)\n",
    "    \n",
    "    return pd.concat(\n",
    "        [ f(table_name) \n",
    "            for f\n",
    "            in [ percentages_per_rating, number_of_reviews, average_reviews_per_product, average_reviews_per_reviewer ]], \n",
    "        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books\n",
      "CDs_and_Vinyl\n",
      "Electronics\n",
      "Movies_and_TV\n",
      "Elapsed time: 146 sec\n"
     ]
    }
   ],
   "source": [
    "metrics = stopwatch(lambda: pd.concat([ all_metrics(table) for table in table_names ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.index.name = 'dataset_name'\n",
    "metrics.to_csv('./metadata/large-datasets-evaluation-metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>reviews_per_product</th>\n",
       "      <th>reviews_per_reviewer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Books</th>\n",
       "      <td>0.036394</td>\n",
       "      <td>0.046652</td>\n",
       "      <td>0.107348</td>\n",
       "      <td>0.249841</td>\n",
       "      <td>0.559765</td>\n",
       "      <td>8898040</td>\n",
       "      <td>24.180639</td>\n",
       "      <td>14.739956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDs and Vinyl</th>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.042430</td>\n",
       "      <td>0.092770</td>\n",
       "      <td>0.224424</td>\n",
       "      <td>0.598288</td>\n",
       "      <td>1097592</td>\n",
       "      <td>17.031982</td>\n",
       "      <td>14.584390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electronics</th>\n",
       "      <td>0.064365</td>\n",
       "      <td>0.048626</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>0.205448</td>\n",
       "      <td>0.597344</td>\n",
       "      <td>1689188</td>\n",
       "      <td>26.812082</td>\n",
       "      <td>8.779427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies and TV</th>\n",
       "      <td>0.061394</td>\n",
       "      <td>0.060329</td>\n",
       "      <td>0.118585</td>\n",
       "      <td>0.225618</td>\n",
       "      <td>0.534074</td>\n",
       "      <td>1697533</td>\n",
       "      <td>33.915388</td>\n",
       "      <td>13.694200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      1         2         3         4         5  \\\n",
       "dataset_name                                                      \n",
       "Books          0.036394  0.046652  0.107348  0.249841  0.559765   \n",
       "CDs and Vinyl  0.042088  0.042430  0.092770  0.224424  0.598288   \n",
       "Electronics    0.064365  0.048626  0.084216  0.205448  0.597344   \n",
       "Movies and TV  0.061394  0.060329  0.118585  0.225618  0.534074   \n",
       "\n",
       "               number_of_reviews  reviews_per_product  reviews_per_reviewer  \n",
       "dataset_name                                                                 \n",
       "Books                    8898040            24.180639             14.739956  \n",
       "CDs and Vinyl            1097592            17.031982             14.584390  \n",
       "Electronics              1689188            26.812082              8.779427  \n",
       "Movies and TV            1697533            33.915388             13.694200  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
