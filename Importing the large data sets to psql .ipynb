{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the large datasets to a postgresql server\n",
    "\n",
    "It is not possible to load the larger data sets in the memory of a local machine therefeore an alternative is to import them to a psql table and query them from there. By adding the right indices this can make the queries fast enough. After this import one can extract some basic statistics using sql and also export smaller portions of the data which can be handled by spark or pandas on a local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzipping the data and converting it to csv format\n",
    "\n",
    "Unfortunately psql does not support an import of record json files therefore we need to convert the data sets to csv. We use here the command line tool [json2csv](https://github.com/jehiah/json2csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: ./data/large-datasets/reviews_CDs_and_Vinyl_5.json: unknown suffix -- ignored\n",
      "rm: cannot remove 'reviews_CDs_and_Vinyl_5': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!gunzip ./data/large-datasets/reviews_CDs_and_Vinyl_5.json.gz ./data/large-datasets/reviews_CDs_and_Vinyl_5.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!json2csv -p -d '|' -k asin,helpful,overall,reviewText,reviewTime,reviewerID,reviewerName,summary,unixReviewTime -i ./data/large-datasets/reviews_CDs_and_Vinyl_5.json -o /tmp/reviews_CDs_and_Vinyl_5.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data in psql\n",
    "\n",
    "To import the data in psql we create a table with the appropriate shape and import form the csv files generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE\r\n"
     ]
    }
   ],
   "source": [
    "!psql -U mariosk -d mariosk -c 'create table cds (asin text, helpful text, overall double precision, reviewText text, reviewTime text, reviewerID text, reviewerName text, summary text, unixReviewTime int);'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n"
     ]
    }
   ],
   "source": [
    "!psql -U mariosk -d mariosk -c 'create index asin ON cds (asin);'\n",
    "!psql -U mariosk -d mariosk -c 'create index overall ON cds (overall);'\n",
    "!psql -U mariosk -d mariosk -c 'create index reviewerID ON cds (reviewerID);'\n",
    "!psql -U mariosk -d mariosk -c 'create index unixReviewTime ON cds (unixReviewTime);'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY 1097592\r\n"
     ]
    }
   ],
   "source": [
    "!psql -U mariosk -d mariosk -c \"copy cds from '/tmp/reviews_CDs_and_Vinyl_5.csv' with (format csv, delimiter '|', header true);\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
