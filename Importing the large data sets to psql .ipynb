{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the large datasets to a postgresql server\n",
    "\n",
    "It is not possible to load the larger data sets in the memory of a local machine therefeore an alternative is to import them to a psql table and query them from there. By adding the right indices this can make the queries fast enough. After this import one can extract some basic statistics using sql and also export smaller portions of the data which can be handled by spark or pandas on a local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzipping the data and converting it to csv format\n",
    "\n",
    "Unfortunately psql does not support an import of record json files therefore we need to convert the data sets to csv. We use here the command line tool [json2csv](https://github.com/jehiah/json2csv).\n",
    "\n",
    "**WARNING:** The following two commands will run for a while, especially the second one. You can expect approximately 1 minute per GB of unzipped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./data/large-datasets/*.gz | grep -Po '.*(?=.gz)' | xargs -I {} gunzip {}.gz {}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls ./data/large-datasets/*.json | grep -Po '.*(?=.json)' | xargs -I {} json2csv -p -d '|' -k asin,helpful,overall,reviewText,reviewTime,reviewerID,reviewerName,summary,unixReviewTime -i {}.json -o {}.csv\n",
    "!rm ./data/large-datasets/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data in psql\n",
    "\n",
    "To import the data in psql we create a table with the appropriate shape and import form the csv files generated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some preparation to run psql transactions and queries in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "\n",
    "db_conf = { \n",
    "    'user': 'mariosk',\n",
    "    'database': 'amazon_reviews'\n",
    "}\n",
    "\n",
    "connection_factory = lambda: pg.connect(user=db_conf['user'], database=db_conf['database'])\n",
    "\n",
    "def transaction(*statements):\n",
    "    try:\n",
    "        connection = connection_factory()\n",
    "        cursor = connection.cursor()\n",
    "        for statement in statements:\n",
    "            cursor.execute(statement)\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "    except pg.DatabaseError as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if connection is not None:\n",
    "            connection.close()\n",
    "    \n",
    "def query(statement):\n",
    "    try:\n",
    "        connection = connection_factory()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(statement)\n",
    "        \n",
    "        header = [ description[0] for description in cursor.description ]\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        cursor.close()\n",
    "        return pd.DataFrame.from_records(rows, columns=header)\n",
    "    except (Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "        return None\n",
    "    finally:\n",
    "        if connection is not None:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tables for with indices for the large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "filenames = [ re.search('reviews_(.*)_5.csv', filename).group(1) \n",
    "    for filename \n",
    "    in sorted(os.listdir('./data/large-datasets'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation \"books\" already exists\n",
      "\n",
      "relation \"cds_and_vinyl\" already exists\n",
      "\n",
      "relation \"electronics\" already exists\n",
      "\n",
      "relation \"movies_and_tv\" already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_table(name):\n",
    "    transaction(\n",
    "        'create table %s (asin text, helpful text, overall double precision, reviewText text, reviewTime text, reviewerID text, reviewerName text, summary text, unixReviewTime int);' % name,\n",
    "        'create index {0}asin ON {0} (asin);'.format('name'),\n",
    "        'create index {0}overall ON {0} (overall);'.format('name'),\n",
    "        'create index {0}reviewerID ON {0} (reviewerID);'.format('name'),\n",
    "        'create index {0}unixReviewTime ON {0} (unixReviewTime);'.format('name'))\n",
    "\n",
    "for filename in filenames:\n",
    "    create_table(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the datasets to psql\n",
    "\n",
    "**WARNING:** The following command will take long time to complete. Estimate ~1 minute for each GB of csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:  extra data after last expected column\n",
      "CONTEXT:  COPY books, line 8791050: \"B00JL1H75A|[0 0]|5|Love this series! I even like Elijah. He has been through a lot and it sounds lik...\"\n",
      "COPY 1097592\n",
      "COPY 1689188\n",
      "COPY 1697533\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/large-datasets | grep -Po '(?<=reviews_).*(?=_5.csv)' | xargs -I {} psql -U mariosk -d amazon_reviews -c \"\\copy {} from './data/large-datasets/reviews_{}_5.csv' with (format csv, delimiter '|', header true);\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_reviews_per_product(table_name):\n",
    "    return (query('''\n",
    "        with distinct_products as (select count(distinct asin) as products from {0}),\n",
    "             reviews_count as (select cast(count(*) as double precision) as reviews from {0})\n",
    "        select reviews / products as reviews_per_product\n",
    "        from distinct_products cross join reviews_count\n",
    "    '''.format(table_name))\n",
    "    .rename(index={0: table_name}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_reviews_per_reviewer(table_name):\n",
    "    return (query('''\n",
    "        with distinct_reviewers as (select count(distinct reviewerID) as reviewers from {0}),\n",
    "             reviews_count as (select cast(count(*) as double precision) as reviews from {0})\n",
    "        select reviews / reviewers as reviews_per_reviewer\n",
    "        from distinct_reviewers cross join reviews_count\n",
    "    '''.format(table_name))\n",
    "    .rename(index={ 0: table_name}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentages_per_rating(table_name):\n",
    "    return (query('''\n",
    "            with rating_counts as (select count(overall) as rating_count from {0} group by overall),\n",
    "                 reviews_count as (select cast(count(*) as double precision) as reviews from {0})\n",
    "            select rating_count / reviews as row\n",
    "            from rating_counts cross join reviews_count\n",
    "        '''.format(table_name))\n",
    "        .transpose()\n",
    "        .rename(index={'row': table_name}, columns=lambda x: str(x + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrics(table_name):\n",
    "    print(table_name)\n",
    "    \n",
    "    return pd.concat(\n",
    "        [ f(table_name) \n",
    "            for f\n",
    "            in [ percentages_per_rating, average_reviews_per_product, average_reviews_per_reviewer ]], \n",
    "        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDs_and_Vinyl\n",
      "Electronics\n",
      "Movies_and_TV\n"
     ]
    }
   ],
   "source": [
    "metrics = pd.concat([ all_metrics(table) for table in filenames if table != 'Books' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.to_csv('./metadata/large-datasets-evaluation-metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>reviews_per_product</th>\n",
       "      <th>reviews_per_reviewer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CDs_and_Vinyl</th>\n",
       "      <td>0.224424</td>\n",
       "      <td>0.042430</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.092770</td>\n",
       "      <td>0.598288</td>\n",
       "      <td>17.031982</td>\n",
       "      <td>14.584390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electronics</th>\n",
       "      <td>0.048626</td>\n",
       "      <td>0.205448</td>\n",
       "      <td>0.064365</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>0.597344</td>\n",
       "      <td>26.812082</td>\n",
       "      <td>8.779427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies_and_TV</th>\n",
       "      <td>0.225618</td>\n",
       "      <td>0.060329</td>\n",
       "      <td>0.118585</td>\n",
       "      <td>0.061394</td>\n",
       "      <td>0.534074</td>\n",
       "      <td>33.915388</td>\n",
       "      <td>13.694200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      1         2         3         4         5  \\\n",
       "CDs_and_Vinyl  0.224424  0.042430  0.042088  0.092770  0.598288   \n",
       "Electronics    0.048626  0.205448  0.064365  0.084216  0.597344   \n",
       "Movies_and_TV  0.225618  0.060329  0.118585  0.061394  0.534074   \n",
       "\n",
       "               reviews_per_product  reviews_per_reviewer  \n",
       "CDs_and_Vinyl            17.031982             14.584390  \n",
       "Electronics              26.812082              8.779427  \n",
       "Movies_and_TV            33.915388             13.694200  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
