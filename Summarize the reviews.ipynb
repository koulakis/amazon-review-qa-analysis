{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the reviews\n",
    "\n",
    "The idea in this solution is to provide a new feature to the customer which will reduce the need to go through several reviews in order to evaluate a product. In order to achieve that, we will attempt to extract the most predictive words or sentences from the ratings and present them in a nice format (e.g. wordcloud).\n",
    "\n",
    "## Implementation steps of a proof of concept\n",
    "\n",
    "- Extract the summaries and split them to words\n",
    "- Keep only the data with ranks 1, 2 -labeled as 0- and 5 -labeled as 1. \n",
    "- Generate tf-idf vector features from the words\n",
    "- Train a binary logistic regression model which predicts the rankings from the vector features\n",
    "- Using this model evaluate each word by generating the features for  it as if it were a whole summary\n",
    "- Order the words by the probability generated by the model to be in the '0' or '1' category\n",
    "- Select the words with highest probability to be '1' as the positive ones\n",
    "- Select the words with highest probability to be '0' as the negative ones\n",
    "- Pick a random set of products and print the top 10 words with highest probabilities (max of positive and negative) on a wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_reviews = (spark\n",
    "    .read\n",
    "    .json('./data/raw_data/reviews_Amazon_Instant_Video_5.json.gz',)\n",
    "    .na\n",
    "    .fill({ 'reviewerName': 'Unknown' }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, udf, trim\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "remove_punctuation = udf(lambda line: re.sub('[^A-Za-z\\s]', '', line))\n",
    "make_binary = udf(lambda rating: 0 if rating in [1, 2] else 1, IntegerType())\n",
    "\n",
    "reviews = (all_reviews\n",
    "    .filter(col('overall').isin([1, 2, 5]))\n",
    "    .withColumn('label', make_binary(col('overall')))\n",
    "    .select(col('label').cast('int'), remove_punctuation('summary').alias('summary'))\n",
    "    .filter(trim(col('summary')) != ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data and balancing skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = reviews.randomSplit([.8, .2], seed=5436L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiply_dataset(dataset, n):\n",
    "    return dataset if n == 1 else dataset.union(multiply_dataset(dataset, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_bad = train.filter('label == 0')\n",
    "reviews_bad_multiplied = multiply_dataset(reviews_bad, 6)\n",
    "\n",
    "reviews_good = train.filter('label == 1')\n",
    "\n",
    "train_reviews = reviews_bad_multiplied.union(reviews_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: predict by distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting 5 stars accuracy: 0.504377044832\n"
     ]
    }
   ],
   "source": [
    "accuracy = reviews_bad_multiplied.count() / float(train_reviews.count())\n",
    "print('Always predicting 5 stars accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='summary', outputCol='words')\n",
    "hashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures')\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "log_regression = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer, \n",
    "    hashing_tf,\n",
    "    idf,\n",
    "    log_regression\n",
    "])\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(hashing_tf.numFeatures, [120000])\n",
    "    .addGrid(log_regression.regParam, [.3])\n",
    "    .addGrid(log_regression.elasticNetParam, [.01])\n",
    "    .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9604014372172316]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = crossval.fit(train_reviews)\n",
    "model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9164916248100313"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "BinaryClassificationEvaluator().evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using model to extract the most predictive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "words = (tokenizer\n",
    "    .transform(reviews)\n",
    "    .select(explode(col('words')).alias('summary')))\n",
    "\n",
    "predictors = (model\n",
    "    .transform(words)\n",
    "    .select(col('summary').alias('word'), 'probability'))\n",
    "\n",
    "first = udf(lambda x: x[0].item(), FloatType())\n",
    "second = udf(lambda x: x[1].item(), FloatType())\n",
    "\n",
    "predictive_words = (predictors\n",
    "   .select(\n",
    "       'word', \n",
    "       second(col('probability')).alias('positive'), \n",
    "       first(col('probability')).alias('negative'))\n",
    "   .groupBy('word')\n",
    "   .agg(\n",
    "       F.max('positive').alias('positive'),\n",
    "       F.max('negative').alias('negative')))\n",
    "\n",
    "positive_predictive_words = (predictive_words\n",
    "    .select(col('word').alias('positive_word'), col('positive').alias('pos_prob'))\n",
    "    .sort('pos_prob', ascending=False))\n",
    "\n",
    "negative_predictive_words = (predictive_words\n",
    "    .select(col('word').alias('negative_word'), col('negative').alias('neg_prob'))\n",
    "    .sort('neg_prob', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_word</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>negative_word</th>\n",
       "      <th>neg_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>five</td>\n",
       "      <td>0.699386</td>\n",
       "      <td>disappointing</td>\n",
       "      <td>0.686565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>excellent</td>\n",
       "      <td>0.691371</td>\n",
       "      <td>meh</td>\n",
       "      <td>0.686261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hilarious</td>\n",
       "      <td>0.689102</td>\n",
       "      <td>theyd</td>\n",
       "      <td>0.680944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lie</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>unwatchable</td>\n",
       "      <td>0.679726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0.687069</td>\n",
       "      <td>mediocre</td>\n",
       "      <td>0.679110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enjoyable</td>\n",
       "      <td>0.684960</td>\n",
       "      <td>boring</td>\n",
       "      <td>0.678281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brilliant</td>\n",
       "      <td>0.681011</td>\n",
       "      <td>eh</td>\n",
       "      <td>0.674466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customers</td>\n",
       "      <td>0.681011</td>\n",
       "      <td>charges</td>\n",
       "      <td>0.674466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>disappoint</td>\n",
       "      <td>0.678679</td>\n",
       "      <td>ditch</td>\n",
       "      <td>0.673823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>outstanding</td>\n",
       "      <td>0.678496</td>\n",
       "      <td>trainwreck</td>\n",
       "      <td>0.673508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>addicted</td>\n",
       "      <td>0.677241</td>\n",
       "      <td>worst</td>\n",
       "      <td>0.672704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rocks</td>\n",
       "      <td>0.676652</td>\n",
       "      <td>sucked</td>\n",
       "      <td>0.671873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amazing</td>\n",
       "      <td>0.675353</td>\n",
       "      <td>nineworst</td>\n",
       "      <td>0.670284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>americanized</td>\n",
       "      <td>0.675353</td>\n",
       "      <td>weakest</td>\n",
       "      <td>0.669122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>0.673847</td>\n",
       "      <td>aggravating</td>\n",
       "      <td>0.668066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>superb</td>\n",
       "      <td>0.672793</td>\n",
       "      <td>protest</td>\n",
       "      <td>0.667827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>favorite</td>\n",
       "      <td>0.672672</td>\n",
       "      <td>letdown</td>\n",
       "      <td>0.667152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>keeps</td>\n",
       "      <td>0.672475</td>\n",
       "      <td>alternatives</td>\n",
       "      <td>0.664778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>endearing</td>\n",
       "      <td>0.672475</td>\n",
       "      <td>swing</td>\n",
       "      <td>0.661212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>classic</td>\n",
       "      <td>0.671540</td>\n",
       "      <td>victimizing</td>\n",
       "      <td>0.660999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   positive_word  pos_prob  negative_word  neg_prob\n",
       "0           five  0.699386  disappointing  0.686565\n",
       "1      excellent  0.691371            meh  0.686261\n",
       "2      hilarious  0.689102          theyd  0.680944\n",
       "3            lie  0.687900    unwatchable  0.679726\n",
       "4        awesome  0.687069       mediocre  0.679110\n",
       "5      enjoyable  0.684960         boring  0.678281\n",
       "6      brilliant  0.681011             eh  0.674466\n",
       "7      customers  0.681011        charges  0.674466\n",
       "8     disappoint  0.678679          ditch  0.673823\n",
       "9    outstanding  0.678496     trainwreck  0.673508\n",
       "10      addicted  0.677241          worst  0.672704\n",
       "11         rocks  0.676652         sucked  0.671873\n",
       "12       amazing  0.675353      nineworst  0.670284\n",
       "13  americanized  0.675353        weakest  0.669122\n",
       "14     fantastic  0.673847    aggravating  0.668066\n",
       "15        superb  0.672793        protest  0.667827\n",
       "16      favorite  0.672672        letdown  0.667152\n",
       "17         keeps  0.672475   alternatives  0.664778\n",
       "18     endearing  0.672475          swing  0.661212\n",
       "19       classic  0.671540    victimizing  0.660999"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.concat([\n",
    "    positive_predictive_words.toPandas().head(n=20),\n",
    "    negative_predictive_words.toPandas().head(n=20) ],\n",
    "    axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
