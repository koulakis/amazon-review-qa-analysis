{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_reviews = (spark\n",
    "#     .read\n",
    "#     .json('../../data/raw_data/reviews_Musical_Instruments_5.json.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, udf, trim\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "remove_punctuation = udf(lambda line: re.sub('[^A-Za-z\\s]', '', line))\n",
    "make_binary = udf(lambda rating: 0 if rating in [1, 2] else 1, IntegerType())\n",
    "\n",
    "reviews = (all_reviews\n",
    "    .na.fill({ 'reviewerName': 'Unknown' })\n",
    "    .filter(col('overall').isin([1, 2, 5]))\n",
    "    .withColumn('label', make_binary(col('overall')))\n",
    "    .select(col('label').cast('int'), remove_punctuation('summary').alias('summary'))\n",
    "    .filter(trim(col('summary')) != ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data and balancing skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = reviews.randomSplit([.8, .2], seed=5436L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiply_dataset(dataset, n):\n",
    "    return dataset if n <= 1 else dataset.union(multiply_dataset(dataset, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_good = train.filter('label == 1')\n",
    "reviews_bad = train.filter('label == 0')\n",
    "\n",
    "reviews_bad_multiplied = multiply_dataset(reviews_bad, reviews_good.count() / reviews_bad.count())\n",
    "\n",
    "\n",
    "train_reviews = reviews_bad_multiplied.union(reviews_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: predict by distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting 5 stars accuracy: 0.506650874636\n"
     ]
    }
   ],
   "source": [
    "accuracy = reviews_good.count() / float(train_reviews.count())\n",
    "print('Always predicting 5 stars accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='summary', outputCol='words')\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer, \n",
    "    HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=120000),\n",
    "    IDF(inputCol='rawFeatures', outputCol='features'),\n",
    "    LogisticRegression(regParam=.3, elasticNetParam=.01)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8499835214632943"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "prediction = model.transform(test)\n",
    "BinaryClassificationEvaluator().evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using model to extract the most predictive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "words = (tokenizer\n",
    "    .transform(reviews)\n",
    "    .select(explode(col('words')).alias('summary')))\n",
    "\n",
    "predictors = (model\n",
    "    .transform(words)\n",
    "    .select(col('summary').alias('word'), 'probability'))\n",
    "\n",
    "first = udf(lambda x: x[0].item(), FloatType())\n",
    "second = udf(lambda x: x[1].item(), FloatType())\n",
    "\n",
    "predictive_words = (predictors\n",
    "   .select(\n",
    "       'word', \n",
    "       second(col('probability')).alias('positive'), \n",
    "       first(col('probability')).alias('negative'))\n",
    "   .groupBy('word')\n",
    "   .agg(\n",
    "       F.max('positive').alias('positive'),\n",
    "       F.max('negative').alias('negative')))\n",
    "\n",
    "positive_predictive_words = (predictive_words\n",
    "    .select(col('word').alias('positive_word'), col('positive').alias('pos_prob'))\n",
    "    .sort('pos_prob', ascending=False))\n",
    "\n",
    "negative_predictive_words = (predictive_words\n",
    "    .select(col('word').alias('negative_word'), col('negative').alias('neg_prob'))\n",
    "    .sort('neg_prob', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_word</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>negative_word</th>\n",
       "      <th>neg_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fullest</td>\n",
       "      <td>0.758080</td>\n",
       "      <td>okay</td>\n",
       "      <td>0.674958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prefer</td>\n",
       "      <td>0.755777</td>\n",
       "      <td>lackluster</td>\n",
       "      <td>0.672741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maybe</td>\n",
       "      <td>0.750025</td>\n",
       "      <td>lightweightattractive</td>\n",
       "      <td>0.666229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>job</td>\n",
       "      <td>0.741355</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>0.665905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>supplies</td>\n",
       "      <td>0.734750</td>\n",
       "      <td>bummer</td>\n",
       "      <td>0.658206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>supposed</td>\n",
       "      <td>0.731578</td>\n",
       "      <td>junk</td>\n",
       "      <td>0.656022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perfect</td>\n",
       "      <td>0.730344</td>\n",
       "      <td>unhappy</td>\n",
       "      <td>0.655910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thinking</td>\n",
       "      <td>0.727673</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.648580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>given</td>\n",
       "      <td>0.727153</td>\n",
       "      <td>crap</td>\n",
       "      <td>0.648168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sometimes</td>\n",
       "      <td>0.724134</td>\n",
       "      <td>limp</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0.723379</td>\n",
       "      <td>busted</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>excellent</td>\n",
       "      <td>0.721763</td>\n",
       "      <td>yuck</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>center</td>\n",
       "      <td>0.721252</td>\n",
       "      <td>cumbersome</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>exactly</td>\n",
       "      <td>0.720202</td>\n",
       "      <td>yuk</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>than</td>\n",
       "      <td>0.720169</td>\n",
       "      <td>yak</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>intended</td>\n",
       "      <td>0.718925</td>\n",
       "      <td>nawwwwwww</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>how</td>\n",
       "      <td>0.718654</td>\n",
       "      <td>hiiiiiiisssssssssssssssssss</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fine</td>\n",
       "      <td>0.717943</td>\n",
       "      <td>nonfunctional</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>help</td>\n",
       "      <td>0.717542</td>\n",
       "      <td>noisey</td>\n",
       "      <td>0.647734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>monkey</td>\n",
       "      <td>0.716862</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>0.647448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   positive_word  pos_prob                negative_word  neg_prob\n",
       "0        fullest  0.758080                         okay  0.674958\n",
       "1         prefer  0.755777                   lackluster  0.672741\n",
       "2          maybe  0.750025        lightweightattractive  0.666229\n",
       "3            job  0.741355                    destroyed  0.665905\n",
       "4       supplies  0.734750                       bummer  0.658206\n",
       "5       supposed  0.731578                         junk  0.656022\n",
       "6        perfect  0.730344                      unhappy  0.655910\n",
       "7       thinking  0.727673                           ok  0.648580\n",
       "8          given  0.727153                         crap  0.648168\n",
       "9      sometimes  0.724134                         limp  0.647734\n",
       "10       awesome  0.723379                       busted  0.647734\n",
       "11     excellent  0.721763                         yuck  0.647734\n",
       "12        center  0.721252                   cumbersome  0.647734\n",
       "13       exactly  0.720202                          yuk  0.647734\n",
       "14          than  0.720169                          yak  0.647734\n",
       "15      intended  0.718925                    nawwwwwww  0.647734\n",
       "16           how  0.718654  hiiiiiiisssssssssssssssssss  0.647734\n",
       "17          fine  0.717943                nonfunctional  0.647734\n",
       "18          help  0.717542                       noisey  0.647734\n",
       "19        monkey  0.716862                 disappointed  0.647448"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.concat([\n",
    "    positive_predictive_words.toPandas().head(n=20),\n",
    "    negative_predictive_words.toPandas().head(n=20) ],\n",
    "    axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
